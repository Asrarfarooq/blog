name: ðŸ—žï¸ Weekly Qubit Tech Roundup

on:
  schedule:
    # Runs every Monday at 9:00 AM CDT (14:00 UTC)
    - cron: '0 14 * * 1' 
  workflow_dispatch:

jobs:
  generate_and_publish:
    runs-on: ubuntu-latest
    permissions:
      contents: write 
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GH_TOKEN_AUTO_COMMIT }}
          fetch-depth: 0 

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: pip install -r requirements.txt

      # -----------------------------------------------
      # --- FIXED: Full Ollama Installation ---
      # -----------------------------------------------
      - name: Install Ollama Client (Fixes 'command not found')
        run: curl -fsSL https://ollama.com/install.sh | sh 

      - name: Pull and Start LLM Service with Health Check
        run: |
          # 1. Ollama was started automatically by the previous step's installer.
          #    We use a health check to wait for the auto-started service to be ready.
          
          echo "Waiting up to 60 seconds for auto-started Ollama service on port 11434..."
          # 2. Robust Health Check Loop
          for i in {1..12}; do
            if curl -s http://localhost:11434 > /dev/null; then
              echo "Ollama API is running!"
              break
            fi
            echo "Attempt $i: Ollama not ready yet. Waiting 5s..."
            sleep 5
            if [ $i -eq 12 ]; then
              echo "Error: Ollama service failed to start within the timeout."
              exit 1
            fi
          done

          # --- FIX ---
          # Add a 5-second delay to allow the Ollama service to fully initialize
          # before we ask it to download a large model. This prevents the race condition.
          echo "Service is up. Waiting 5 seconds for full initialization..."
          sleep 5

          # 3. Pull the required model (phi3:3.8b-mini-4k-instruct)
          echo "Pulling phi3:3.8b-mini-4k-instruct model..."
          # We use the now-running service to pull the model
          ollama pull phi3:3.8b-mini-4k-instruct
          
          echo "Model ready. Proceeding to content generation."

      - name: Run Content Generation Script (Weekly)
        run: python automation_script.py --type weekly
        env:
          GH_TOKEN_AUTO_COMMIT: ${{ secrets.GH_TOKEN_AUTO_COMMIT }}