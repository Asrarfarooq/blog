name: ü§ñ Daily Qubit AI/ML Post

on:
  schedule:
    # Runs twice daily at 8:30 AM CDT (13:30 UTC) and 3:30 PM CDT (20:30 UTC)
    - cron: '30 13,20 * * *' 
  workflow_dispatch: 

jobs:
  generate_and_publish:
    runs-on: ubuntu-latest
    permissions:
      contents: write 
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GH_TOKEN_AUTO_COMMIT }} 
          fetch-depth: 0 
          
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: pip install -r requirements.txt
        
      # -----------------------------------------------
      # --- FINAL FIX: CONFIGURE, INSTALL, and PULL ---
      # -----------------------------------------------
      - name: Configure, Install, and Pull Ollama
        run: |
          # 1. Configure the Ollama service to listen on all interfaces.
          # This is a robust setting for CI/CD environments and resolves internal networking issues.
          export OLLAMA_HOST=0.0.0.0
          
          # 2. Install Ollama. The installer will respect the OLLAMA_HOST variable for the systemd service.
          curl -fsSL https://ollama.com/install.sh | sh
          
          # 3. Wait for the API to be fully responsive on the new host.
          echo "Waiting up to 60 seconds for auto-started Ollama service..."
          for i in {1..12}; do
            # Note: We now curl 0.0.0.0 to match the service configuration
            if curl -s --fail http://0.0.0.0:11434/api/tags > /dev/null; then
              echo "Ollama API is fully responsive!"
              break
            fi
            echo "Attempt $i: Ollama not ready yet. Waiting 5s..."
            sleep 5
            if [ $i -eq 12 ]; then
              echo "Error: Ollama service failed to start within the timeout."
              sudo journalctl -u ollama.service --no-pager -n 100
              exit 1
            fi
          done

          # 4. Pull the model using the REST API, now targeting 0.0.0.0.
          echo "Pulling 'phi3:3.8b-mini-4k-instruct' model directly via REST API..."
          curl http://0.0.0.0:11434/api/pull -d '{
            "name": "phi3:3.8b-mini-4k-instruct",
            "stream": false
          }'

          # 5. Verify the model was downloaded successfully.
          echo ""
          echo "Verifying model installation..."
          if ollama list | grep -q "phi3:3.8b-mini-4k-instruct"; then
            echo "‚úÖ Model successfully pulled and verified."
          else
            echo "‚ùå VERIFICATION FAILED. Model was not found after the API pull command."
            echo "Dumping Ollama service logs for final diagnostics:"
            sudo journalctl -u ollama.service --no-pager --all -n 200
            exit 1
          fi

          echo "Model is ready. Proceeding to content generation."
          
      - name: Run Content Generation Script (Daily)
        run: python automation_script.py --type daily
        env:
          GH_TOKEN_AUTO_COMMIT: ${{ secrets.GH_TOKEN_AUTO_COMMIT }}